# Measuring wages
new_data <- train_filled %>%
mutate(Age_Group = cut(DAYS_BIRTH / 365,
breaks = c(0, 25, 35, 45, 55, 65, 100),
labels = c("18-25", "26-35", "36-45", "46-55", "56-65", "66+"),
right = FALSE)) %>%
dplyr::select(Age_Group, TARGET)  # Select only the Age Group and TARGET columns
# Bar plot with two bars per age group for Loan Repayment Status
ggplot(new_data, aes(x = Age_Group, fill = factor(TARGET))) +
geom_bar(position = "dodge") +  # Use dodge for side-by-side bars
scale_fill_manual(values = c("#4F81BD", "#B0B0B0"), labels = c("On Time", "Difficulties")) +
labs(x = "Age Group", y = "Count", title = "Loan Repayment Status by Age Group") +
theme_minimal(base_size = 13) +
theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
axis.text.y = element_text(angle = 0, hjust = 1),  # Ensure labels on y-axis are horizontal
legend.position = "top"  # Move the legend to the top
) +
scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) +
guides(fill = guide_legend(title = "Repayment"))  # Add title for legend +
# Create bins for each column
train_bins <- train_filled %>%
mutate(
AMT_INCOME_TOTAL_BIN = cut(AMT_INCOME_TOTAL, breaks = c(0, 100000, 150000, 200000, 250000, 300000, Inf),
labels = c("0-100k", "100k-150k", "150k-200k", "200k-250k", "250k-300k", "300k+")),
AMT_CREDIT_BIN = cut(AMT_CREDIT, breaks = c(0, 200000, 400000, 600000, 800000, 1000000, Inf),
labels = c("0-200k", "200k-400k", "400k-600k", "600k-800k", "800k-1M", "1M+"))
)
# Aggregate data to count TARGET values for each bin
income_target <- train_bins %>%
group_by(AMT_INCOME_TOTAL_BIN, TARGET) %>%
summarise(Count = n(), .groups = "drop")
credit_target <- train_bins %>%
group_by(AMT_CREDIT_BIN, TARGET) %>%
summarise(Count = n(), .groups = "drop")
# Income total
ggplot(income_target, aes(x = AMT_INCOME_TOTAL_BIN, y = Count, fill = factor(TARGET))) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = c("#4F81BD", "#B0B0B0"), labels = c("On Time", "Difficulties")) +
labs(title = "Number of Repayment by Income Range",
x = "Income Range",
y = "Count",
fill = "TARGET") +
theme_minimal()
# Credit
ggplot(credit_target, aes(x = AMT_CREDIT_BIN, y = Count, fill = factor(TARGET))) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = c("#4F81BD", "#B0B0B0"), labels = c("On Time", "Difficulties")) +
labs(title = "Number of Repayment by Credit Range",
x = "Credit Range",
y = "Count",
fill = "TARGET") +
theme_minimal()
# Calculate proportions for each bin
income_target_prop <- train_bins %>%
group_by(AMT_INCOME_TOTAL_BIN, TARGET) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(AMT_INCOME_TOTAL_BIN) %>%
mutate(Proportion = Count / sum(Count))
credit_target_prop <- train_bins %>%
group_by(AMT_CREDIT_BIN, TARGET) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(AMT_CREDIT_BIN) %>%
mutate(Proportion = Count / sum(Count))
# Plotting proportions
# Percentage bar chart for Income Range
ggplot(income_target_prop, aes(fill = factor(TARGET), y = Proportion, x = AMT_INCOME_TOTAL_BIN)) +
geom_bar(position = "fill", stat = "identity") +
scale_fill_manual(values = c("#4F81BD", "#B0B0B0"), labels = c("On Time", "Difficulties")) +
scale_y_continuous(labels = scales::percent_format(scale = 1)) +
labs(title = "Proportion of repayment by Income Range",
x = "Income Range",
y = "Proportion",
fill = "TARGET") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Percentage bar chart for Credit Range
ggplot(credit_target_prop, aes(fill = factor(TARGET), y = Proportion, x = AMT_CREDIT_BIN)) +
geom_bar(position = "fill", stat = "identity") +
scale_fill_manual(values = c("#4F81BD", "#B0B0B0"), labels = c("On Time", "Difficulties")) +
scale_y_continuous(labels = scales::percent_format(scale = 1)) +
labs(title = "Proportion of repayment by Credit Range",
x = "Credit Range",
y = "Proportion",
fill = "TARGET") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Identify all FLAG_DOCUMENT_X columns
flag_columns <- grep("^FLAG_DOCUMENT_", names(train_filled), value = TRUE)
# Create a new column with the count of documents provided (sum of flags)
train_filled$DOCUMENT_COUNT <- rowSums(train_filled[, flag_columns])
# Remove the FLAG_DOCUMENT_X columns from the dataset
train_filled <- train_filled[, !names(train_filled) %in% flag_columns]
# Dropping and mutate column based on domain knowledge
train_clean <-train_filled %>%
dplyr::select(-AMT_REQ_CREDIT_BUREAU_HOUR, -AMT_REQ_CREDIT_BUREAU_DAY, -AMT_REQ_CREDIT_BUREAU_WEEK, -AMT_REQ_CREDIT_BUREAU_MON, -AMT_REQ_CREDIT_BUREAU_YEAR, -WEEKDAY_APPR_PROCESS_START, -NAME_INCOME_TYPE, -FLAG_MOBIL) %>%
mutate(DAY_EMPLOYED_PERCENT = DAYS_EMPLOYED / DAYS_BIRTH) %>%
mutate(AGES = DAYS_BIRTH/365)  %>%
dplyr::select(-SK_ID_CURR, -DAYS_BIRTH) %>%
mutate(CREDIT_INCOME_PERCENT = AMT_CREDIT / AMT_INCOME_TOTAL) %>%
filter(CODE_GENDER != "XNA")
# Change the categorical column into numerical based on domain knowledge
train_clean$NAME_EDUCATION_TYPE <- as.numeric(factor(train_clean$NAME_EDUCATION_TYPE, labels = c("Secondary", "Lower Secondary", "Incomplete Higher", "Higher Education", "Academic Degree")))
train_clean$OCCUPATION_TYPE <- as.numeric(table(train_clean$OCCUPATION_TYPE )[train_clean$OCCUPATION_TYPE ])
train_clean$NAME_TYPE_SUITE <- as.numeric(table(train_clean$NAME_TYPE_SUITE )[train_clean$NAME_TYPE_SUITE ])
train_clean$NAME_HOUSING_TYPE <- as.numeric(table(train_clean$NAME_HOUSING_TYPE )[train_clean$NAME_HOUSING_TYPE ])
train_clean$NAME_CONTRACT_TYPE <- as.numeric(table(train_clean$NAME_CONTRACT_TYPE )[train_clean$NAME_CONTRACT_TYPE])
train_clean$ORGANIZATION_TYPE <- as.numeric(table(train_clean$ORGANIZATION_TYPE )[train_clean$ORGANIZATION_TYPE])
train_clean$NAME_FAMILY_STATUS <- as.numeric(table(train_clean$NAME_FAMILY_STATUS )[train_clean$NAME_FAMILY_STATUS])
# Do Hot-one Enconding for the remain variable
dummy_vars <- dummyVars(~ ., data = train_clean, fullRank = TRUE)
train_before <- data.frame(predict(dummy_vars, train_clean))
# Separate the numerical columns
numerical_data <- train_before %>%
dplyr::select_if(is.numeric)  # Selects only numerical columns
# Calculate the initial correlation matrix
corr_matrix <- cor(numerical_data)
# Set a high correlation threshold
threshold <- 0.8
# Create a copy of the correlation matrix, setting correlations below the threshold to NA
corr_matrix[abs(corr_matrix) < threshold] <- NA  # Set correlations below the threshold to NA
# Function to check if all non-NA values are 1
all_one_or_na <- function(x) all(x[!is.na(x)] == 1)
# Remove columns and rows that contain only 1 and NA
corr_matrix <- corr_matrix[, !apply(corr_matrix, 2, all_one_or_na)]
corr_matrix <- corr_matrix[!apply(corr_matrix, 1, all_one_or_na), ]
# Identify variables with at least one significant correlation
significant_vars <- apply(!is.na(corr_matrix), 1, any)
# Filter out variables without significant correlations
filtered_corr_matrix <- corr_matrix[significant_vars, significant_vars]
# Convert the filtered correlation matrix into long format
cor_long <- melt(filtered_corr_matrix, na.rm = TRUE)
# Plot the heatmap
ggplot(data = cor_long, aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "#084594", high = "#B22222", mid = "white",
limit = c(-1, 1), midpoint = 0,
name = "Correlation") +
labs(title = paste("Filtered Correlation Matrix (Corr >= ", threshold, ")", sep = ""),
x = "",
y = "") +
theme_minimal(base_size = 13) +
theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1, size = 7, vjust = 1),
axis.text.y = element_text(hjust = 1, size = 7, vjust = 1))
# Change the TARGET variable into factor for Elastic Net
train_before$TARGET <- factor(train_before$TARGET, levels = c(0, 1))
# Ensure that levels of TARGET are valid R variable names
levels(train_before$TARGET) <- make.names(levels(train_before$TARGET))
# Parallel Processing for faster training
#cl <- makeCluster(detectCores()-1)
#registerDoParallel()
# Define custom Precision function with X0 as positive
precisionSummary <- function(data, lev = NULL, model = NULL) {
data$obs <- factor(data$obs)
data$pred <- factor(data$pred)
precision <- posPredValue(data$obs, data$pred, positive = "X1")
out <- c(Precision = precision)
return(out)
}
# Define train control for cross validation and scale the TARGET variable
# train_control <- trainControl(
#   method = "cv",                  # Cross-validation
#   number = 10,                     # 5-fold CV
#   classProbs = TRUE,              # Enable class probabilities
#   summaryFunction = precisionSummary,  # Use custom Precision function
#   sampling = "down"
#   )
# Elastic Net model training
#  elastic_net_model <- train(
#    TARGET ~ .,                    # Formula for predictors and outcome
#    data = train_before,                    # Dataset
#    method = "glmnet",              # Elastic Net
#    metric = "Precision",           # Optimize for Precision
#    trControl = train_control,      # Training control
#    tuneGrid = expand.grid(         # Tuning grid for alpha and lambda
#      alpha = seq(0.1, 0.5, by = 0.1),
#      lambda = seq(0.01, 0.05, by = 0.01)),
#    preProcess = c("center", "scale"))
#
# stopCluster(cl)
# registerDoSEQ()
# Save the model
# saveRDS(elastic_net_model, "elastic.rds")
#Read the model
elastic_net_model <- readRDS("elastic.rds")
# Print model results
print(elastic_net_model)
# Extract coefficients of the best model
best_model <- elastic_net_model$finalModel
best_lambda <- elastic_net_model$bestTune$lambda
selected_features <- coef(best_model, s = best_lambda)
coefficients <- as.matrix(selected_features)
# Identify non-zero coefficients (i.e., variables retained in the model)
non_zero_coefficients <- coefficients[coefficients != 0, , drop = FALSE]
print(non_zero_coefficients)
# Convert into matrix form for significant variables
non_zero_coefficients <- as.matrix(non_zero_coefficients)
selected_features <- colnames(train_before)[which(non_zero_coefficients[-1,] != 0)]
# Subset the original dataset to include only final features for PCA
final_filtered_data <- train_before %>%
dplyr::select(selected_features)
x <- final_filtered_data %>%
dplyr::select(-TARGET)
# Calculate the initial correlation matrix
corr_matrix <- cor(x)
# Set a high correlation threshold
threshold <- 0.8
# Create a copy of the correlation matrix, setting correlations below the threshold to NA
corr_matrix[abs(corr_matrix) < threshold] <- NA  # Set correlations below the threshold to NA
# Function to check if all non-NA values are 1
all_one_or_na <- function(x) all(x[!is.na(x)] == 1)
# Remove columns and rows that contain only 1 and NA
corr_matrix <- corr_matrix[, !apply(corr_matrix, 2, all_one_or_na)]
corr_matrix <- corr_matrix[!apply(corr_matrix, 1, all_one_or_na), ]
# Identify variables with at least one significant correlation
significant_vars <- apply(!is.na(corr_matrix), 1, any)
# Filter out variables without significant correlations
filtered_corr_matrix <- corr_matrix[significant_vars, significant_vars]
# Convert the filtered correlation matrix into long format
cor_long <- melt(filtered_corr_matrix, na.rm = TRUE)
# Plot the heatmap
ggplot(data = cor_long, aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "#084594", high = "#B22222", mid = "white",
limit = c(-1, 1), midpoint = 0,
name = "Correlation") +
labs(title = paste("Filtered Correlation Matrix (Corr >= ", threshold, ")", sep = ""),
x = "",
y = "") +
theme_minimal(base_size = 13) +
theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1, size = 7, vjust = 1),
axis.text.y = element_text(hjust = 1, size = 7, vjust = 1))
# Remove correlated variable
final_filtered_data <- final_filtered_data %>%
dplyr::select(-CNT_CHILDREN, -FLAG_EMP_PHONE, -REGION_RATING_CLIENT)
# Filter x variable
final_filtered_data_x <- final_filtered_data[, -which(names(final_filtered_data) == "TARGET")]
# Apply PCA to the standardized data
pca_result <- prcomp(final_filtered_data_x, center = TRUE, scale. = TRUE)
# Summary of PCA results
summary(pca_result)
# Get explained variance
explained_variance <- summary(pca_result)$importance[2, ]
# Cumulative explained variance
cumulative_variance <- cumsum(explained_variance)
# Plot the cumulative variance
plot(cumulative_variance, type = "b", xlab = "Number of Components", ylab = "Cumulative Explained Variance",
main = "Cumulative Explained Variance by Principal Components")
# Select the PC for our data
select_train <- pca_result$x[, 1:25]
# Combine the TARGET variable and our principle component
cleaned_matrix <- bind_cols(TARGET = as.factor(final_filtered_data$TARGET), select_train)
# Transform it to DF for trainning
pca_transformed_df <- as.data.frame(cleaned_matrix)
test_filled <- test_data %>%
dplyr::select(-all_of(columns_to_drop)) %>%
mutate(
EXT_SOURCE_3 = ifelse(is.na(EXT_SOURCE_3), median(EXT_SOURCE_3, na.rm = TRUE), EXT_SOURCE_3),
AMT_REQ_CREDIT_BUREAU_HOUR = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_HOUR), 0, AMT_REQ_CREDIT_BUREAU_HOUR),
AMT_REQ_CREDIT_BUREAU_DAY = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_DAY), 0, AMT_REQ_CREDIT_BUREAU_DAY),
AMT_REQ_CREDIT_BUREAU_WEEK = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_WEEK), 0, AMT_REQ_CREDIT_BUREAU_WEEK),
AMT_REQ_CREDIT_BUREAU_MON = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_MON), median(AMT_REQ_CREDIT_BUREAU_MON, na.rm = TRUE), AMT_REQ_CREDIT_BUREAU_MON),
AMT_REQ_CREDIT_BUREAU_QRT = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_QRT), median(AMT_REQ_CREDIT_BUREAU_QRT, na.rm = TRUE), AMT_REQ_CREDIT_BUREAU_QRT),
AMT_REQ_CREDIT_BUREAU_YEAR = ifelse(is.na(AMT_REQ_CREDIT_BUREAU_YEAR), median(AMT_REQ_CREDIT_BUREAU_YEAR, na.rm = TRUE), AMT_REQ_CREDIT_BUREAU_YEAR),
OBS_30_CNT_SOCIAL_CIRCLE = ifelse(is.na(OBS_30_CNT_SOCIAL_CIRCLE), median(OBS_30_CNT_SOCIAL_CIRCLE, na.rm = TRUE), OBS_30_CNT_SOCIAL_CIRCLE),
DEF_30_CNT_SOCIAL_CIRCLE = ifelse(is.na(DEF_30_CNT_SOCIAL_CIRCLE), median(DEF_30_CNT_SOCIAL_CIRCLE, na.rm = TRUE), DEF_30_CNT_SOCIAL_CIRCLE),
OBS_60_CNT_SOCIAL_CIRCLE = ifelse(is.na(OBS_60_CNT_SOCIAL_CIRCLE), median(OBS_60_CNT_SOCIAL_CIRCLE, na.rm = TRUE), OBS_60_CNT_SOCIAL_CIRCLE),
DEF_60_CNT_SOCIAL_CIRCLE = ifelse(is.na(DEF_60_CNT_SOCIAL_CIRCLE), median(DEF_60_CNT_SOCIAL_CIRCLE, na.rm = TRUE), DEF_60_CNT_SOCIAL_CIRCLE),
EXT_SOURCE_2 = ifelse(is.na(EXT_SOURCE_2), median(EXT_SOURCE_2, na.rm = TRUE), EXT_SOURCE_2),
AMT_GOODS_PRICE = ifelse(is.na(AMT_GOODS_PRICE), median(AMT_GOODS_PRICE, na.rm = TRUE), AMT_GOODS_PRICE),
AMT_ANNUITY = ifelse(is.na(AMT_ANNUITY), median(AMT_ANNUITY, na.rm = TRUE), AMT_ANNUITY),
CNT_FAM_MEMBERS = ifelse(is.na(CNT_FAM_MEMBERS), median(CNT_FAM_MEMBERS, na.rm = TRUE), CNT_FAM_MEMBERS),
DAYS_LAST_PHONE_CHANGE = ifelse(is.na(DAYS_LAST_PHONE_CHANGE), median(DAYS_LAST_PHONE_CHANGE, na.rm = TRUE), DAYS_LAST_PHONE_CHANGE)
) %>%
mutate(OCCUPATION_TYPE = ifelse(is.na(OCCUPATION_TYPE), "Unknown", OCCUPATION_TYPE)) %>%
mutate(NAME_TYPE_SUITE = ifelse(is.na(NAME_TYPE_SUITE), "Unaccompanied", NAME_TYPE_SUITE)) %>%
mutate(across(c(DAYS_BIRTH, DAYS_LAST_PHONE_CHANGE, DAYS_EMPLOYED, DAYS_ID_PUBLISH, DAYS_REGISTRATION), abs))
# Reorder and relabel the education categories
test_filled$NAME_EDUCATION_TYPE <- factor(
test_filled$NAME_EDUCATION_TYPE,
levels = c("Secondary / secondary special", "Lower secondary", "Incomplete higher",
"Higher education", "Academic degree"),
labels = c("Secondary", "Lower Secondary", "Incomplete Higher", "Higher Education", "Academic Degree")
)
# Create a new column with the count of documents provided (sum of flags)
test_filled$DOCUMENT_COUNT <- rowSums(test_filled[, flag_columns])
# Remove the FLAG_DOCUMENT_X columns from the dataset
test_filled <- test_filled[, !names(test_filled) %in% flag_columns]
# Dropping and mutate column based on domain knowledge
test_clean <-test_filled %>%
dplyr::select(-AMT_REQ_CREDIT_BUREAU_HOUR, -AMT_REQ_CREDIT_BUREAU_DAY, -AMT_REQ_CREDIT_BUREAU_WEEK, -AMT_REQ_CREDIT_BUREAU_MON, -AMT_REQ_CREDIT_BUREAU_YEAR, -WEEKDAY_APPR_PROCESS_START, -NAME_INCOME_TYPE, -FLAG_MOBIL) %>%
mutate(DAY_EMPLOYED_PERCENT = DAYS_EMPLOYED / DAYS_BIRTH) %>%
mutate(AGES = DAYS_BIRTH/365)  %>%
dplyr::select(-SK_ID_CURR, -DAYS_BIRTH) %>%
mutate(CREDIT_INCOME_PERCENT = AMT_CREDIT / AMT_INCOME_TOTAL) %>%
filter(CODE_GENDER != "XNA")
# Change the categorical column into numerical based on domain knowledge
test_clean$NAME_EDUCATION_TYPE <- as.numeric(factor(test_clean$NAME_EDUCATION_TYPE, labels = c("Secondary", "Lower Secondary", "Incomplete Higher", "Higher Education", "Academic Degree")))
test_clean$OCCUPATION_TYPE <- as.numeric(table(test_clean$OCCUPATION_TYPE )[test_clean$OCCUPATION_TYPE ])
test_clean$NAME_TYPE_SUITE <- as.numeric(table(test_clean$NAME_TYPE_SUITE )[test_clean$NAME_TYPE_SUITE ])
test_clean$NAME_HOUSING_TYPE <- as.numeric(table(test_clean$NAME_HOUSING_TYPE )[test_clean$NAME_HOUSING_TYPE ])
test_clean$NAME_CONTRACT_TYPE <- as.numeric(table(test_clean$NAME_CONTRACT_TYPE )[test_clean$NAME_CONTRACT_TYPE])
test_clean$ORGANIZATION_TYPE <- as.numeric(table(test_clean$ORGANIZATION_TYPE )[test_clean$ORGANIZATION_TYPE])
test_clean$NAME_FAMILY_STATUS <- as.numeric(table(test_clean$NAME_FAMILY_STATUS )[test_clean$NAME_FAMILY_STATUS])
# Do Hot-one Enconding for the remain variable
dummy_vars <- dummyVars(~ ., data = test_clean, fullRank = TRUE)
test_before <- data.frame(predict(dummy_vars, test_clean))
# Change the TARGET variable into factor for Elastic Net
test_before$TARGET <- factor(test_before$TARGET, levels = c(0, 1))
# Ensure that levels of TARGET are valid R variable names
levels(test_before$TARGET) <- make.names(levels(test_before$TARGET))
# Subset the original dataset to include only final features for PCA
final_filtered_test <- test_before %>%
dplyr::select(selected_features)
# Remove correlated variable
final_filtered_test <- final_filtered_test %>%
dplyr::select(-CNT_CHILDREN, -FLAG_EMP_PHONE, -REGION_RATING_CLIENT)
final_filtered_test <- na.omit(final_filtered_test)  # Removes rows with NA values
# Filter
final_filtered_test_x <- final_filtered_test[, -which(names(final_filtered_test) == "TARGET")]
# Apply PCA to the standardized data
pca_result_test <- prcomp(final_filtered_test_x, center = TRUE, scale. = TRUE)
# Select the PC for our data
select_train_test <- pca_result_test$x[, 1:25]
# Combine the TARGET variable and our principle component
cleaned_matrix_test <- bind_cols(select_train_test, TARGET = as.factor(final_filtered_test$TARGET))
# Transform it to DF for trainning
pca_transformed_test <- as.data.frame(cleaned_matrix_test)
# Getting independant and dependant variable in test data
x_test <- pca_transformed_test %>%
dplyr::select(-TARGET)
y_test <- pca_transformed_test$TARGET
# Parallel Processing for faster training
cl <- makeCluster(detectCores()-1)
registerDoParallel()
# Define train control with custom Precision function
train_control1 <- trainControl(
method = "cv",                   # Cross-validation
number = 10,                      # Number of folds for CV
classProbs = TRUE,                # To compute probabilities
summaryFunction = precisionSummary, # Use custom Precision function
sampling = "down",
allowParallel = TRUE
)
# Train logistic regression model
logistic_model_pca <- train(
TARGET ~ .,    # Formula (target ~ predictors)
data = pca_transformed_df,                # Training data
method = "glm",                   # Generalized Linear Model
family = "binomial",              # Logistic regression
trControl = train_control1,        # Cross-validation settings
metric = "Precision",
preProcess = c("center", "scale")
)
stopCluster(cl)
registerDoSEQ()
# View model results
print(logistic_model_pca)
# Make predictions on the test set
lr_predictions <- predict(logistic_model_pca, newdata = x_test)
# Assuming the true labels are stored in test_target
lr_conf_matrix <- confusionMatrix(lr_predictions, y_test)
lr_precision <- lr_conf_matrix$byClass["Specificity"]
#   cl <- makeCluster(detectCores()-1)
#   registerDoParallel()
#
# Define train control with custom Precision function
# train_control2 <- trainControl(
#   method = "cv",                   # Cross-validation
#   number = 5,                      # Number of folds for CV
#   summaryFunction = precisionSummary, # Use custom Precision function
#   sampling = "down",
#   verboseIter = TRUE,
#   allowParallel = TRUE
#   )
#
# tuneGrid <- expand.grid(
#   mtry = seq(2,10, by = 2)
# )
#
# # Train the Random Forest model
# rf_model_pca <- train(
#   TARGET ~ .,                   # Formula (target ~ predictors)
#   data = pca_transformed_df,              # Training data
#   method = "rf",                 # Random Forest method
#   trControl = train_control2,     # Cross-validation control
#   metric = "Precision",              # Optimize for ROC (AUC)
#   tuneGrid = tuneGrid,
#   ntree = 100,
#   preProcess = c("center", "scale")# Tune a few hyperparameters (adjust if needed)
# )
#
# stopCluster(cl)
# registerDoSEQ()
#saveRDS(rf_model_pca, "rf_model_pca.rds")
rf_model_pca <- readRDS("rf_model_pca.rds")
# Print the model results
print(rf_model_pca)
# Make predictions on the test set
rf_predictions <- predict(rf_model_pca, newdata = x_test)
# Assuming the true labels are stored in test_target
rf_conf_matrix <- confusionMatrix(rf_predictions, y_test)
rf_precision <- rf_conf_matrix$byClass["Specificity"]
# # Set up parallel processing
# cl <- makeCluster(detectCores() - 1) # Use one less core to avoid overload
# registerDoParallel(cl)
#
# # Set up train control with custom precision function
# train_control3 <- trainControl(
#   method = "cv",                         # Cross-validation
#   number = 5,                            # 5-fold cross-validation
#   summaryFunction = precisionSummary,     # Use custom precision function
#   sampling = "down",                     # Down-sample the majority class to balance classes
#   allowParallel = TRUE,                    # Enable parallelization
# )
#
# # Hyperparameter grid for tuning XGBoost
# tuneGrid <- expand.grid(
#   nrounds = 50,                     # Number of boosting rounds (trees)
#   max_depth = c(3, 6),                      # Maximum depth of trees
#   eta = c(0.1, 0.3),                        # Learning rate
#   gamma = c(0, 0.1),                        # Minimum loss reduction required to make a further partition
#   colsample_bytree = c(0.6, 0.8),           # Fraction of features used for each tree
#   min_child_weight = c(1, 5),                # Minimum sum of instance weight (hessian) needed in a child
#   subsample = c(0.7, 0.8)                   # Fraction of samples used for training each tree
# )
#
# # Train the XGBoost model using caret
# xgb_model <- train(
#   TARGET ~ .,                   # Formula (TARGET as the target variable)
#   data = pca_transformed_df,     # Your PCA-transformed data frame
#   method = "xgbTree",            # Specify XGBoost model (tree-based)
#   trControl = train_control3,    # Cross-validation control with custom precision function
#   tuneGrid = tuneGrid,           # Hyperparameter tuning grid
#   metric = "Precision",          # Optimize for Precision
#   preProcess = c("center", "scale")  # Preprocess data: standardize features
# )
#
# # Stop the parallel cluster
# stopCluster(cl)
#registerDoSEQ()  # Disable parallelism for further operations
#saveRDS(xgb_model, "xgb_model_pca.rds")
xgb_model_pca <- readRDS("xgb_model_pca.rds")
# Print the model results
print(xgb_model_pca)
# Make predictions on the test set
xgb_predictions <- predict(xgb_model_pca, newdata = x_test)
# Assuming the true labels are stored in test_target
xgb_conf_matrix <- confusionMatrix(xgb_predictions, y_test)
xgb_precision <- xgb_conf_matrix$byClass["Specificity"]
# Create a data frame with the model names and their corresponding precision
precision_table <- data.frame(
Model = c("Random Forest (RF)", "Extreme Gradient Boosting (XGB)", "Logistic Regression (LR)"),
Precision = c(rf_precision, xgb_precision, lr_precision)
)
# Order the data frame by decreasing precision
precision_table <- precision_table[order(-precision_table$Precision), ]
# Print the table using knitr::kable
kable(precision_table, caption = "Model Precision")
# Get the loadings (rotation matrix)
loadings_matrix <- pca_result$rotation
# Select the first 12 principal components
selected_loadings <- loadings_matrix[, 1:12]
# Create a cleaner table display with kable
kable(selected_loadings,
caption = "Loadings for the First 12 Principal Components (PCs)",
digits = 3,  # Format numeric values to 3 decimal places
col.names = paste("PC", 1:12))  # Provide descriptive column names
summary(logistic_model_pca)
xgb_model <- xgb_model_pca$finalModel
importance_matrix <- xgb.importance(model = xgb_model)
# Top 10 important features
top_10 <- head(importance_matrix, 10)
# Professional plot with color and styling
ggplot(top_10, aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_gradient(low = "#99c2ff", high = "#084594") +  # Color gradient
theme_minimal(base_size = 13) +
theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1, size = 7, vjust = 1),
axis.text.y = element_text(hjust = 1, size = 7, vjust = 1),
legend.position = "None") +
labs(
title = "Top 10 Feature Importance (XGBoost)",
x = "Feature",
y = "Importance"
)
# Extract the loadings for the 12th principal component
selected_loadings <- loadings_matrix[, 12]
kable(selected_loadings, col.names = c("Feature", "Loading Value"), caption = "Loadings for the 12th Principal Component (PC12)")
# Extract the loadings for the 12th principal component
selected_loadings <- loadings_matrix[, 2]
kable(selected_loadings, col.names = c("Feature", "Loading Value"), caption = "Loadings for the 2nd Principal Component (PC2)")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
variable <- read.csv("HomeCredit_columns_description.csv"
variable <- read.csv("HomeCredit_columns_description.csv")
view(variable)
variable <- read.csv("HomeCredit_columns_description.csv")
view(variable)
variable <- variable %>%
filter(Table == "application_{train|test}.csv")
variable <- read.csv("HomeCredit_columns_description.csv")
variable <- variable %>%
filter(Table == "application_{train|test}.csv")
view(variable)
variable <- read.csv("HomeCredit_columns_description.csv")
variable <- variable %>%
filter(Table == "application_{train|test}.csv")
view(variable)
write.csv(variable, "variabledes.csv")
